{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsQA43+1xnDpXDJEOLPjhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HosseinEyvazi/Voice-AI-Booklet/blob/main/Voice_Sec1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1) From signal → sound → voice\n",
        "\n",
        "* **Signal**: a physical waveform (air pressure changes).\n",
        "* **Sound**: any audible signal.\n",
        "* **Voice**: human sound with vocal tract patterns (pitch, timbre, phonemes).\n",
        "  In ML we go from the **raw waveform** → **features** (e.g., mel-spectrograms) → **models** that map audio⇄text.\n",
        "\n",
        "---\n",
        "\n",
        "# 2) Speech-to-Text (STT) from microphone with `speech_recognition` (Google Web Speech API)\n",
        "\n",
        "### Install\n",
        "\n",
        "```bash\n",
        "pip install SpeechRecognition pyaudio\n",
        "# if PyAudio wheels are hard to install on your OS:\n",
        "# pip install sounddevice\n",
        "# pip install soundfile\n",
        "```\n",
        "\n",
        "### Code (microphone → text)\n",
        "\n",
        "```python\n",
        "import speech_recognition as sr\n",
        "\n",
        "def listen_and_transcribe(lang=\"en-US\"):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Adjusting for ambient noise…\")\n",
        "        r.adjust_for_ambient_noise(source, duration=1)\n",
        "        print(\"Speak now!\")\n",
        "        audio = r.listen(source, timeout=5, phrase_time_limit=15)\n",
        "\n",
        "    try:\n",
        "        # Uses Google’s free web API (rate-limited; needs internet; not for production)\n",
        "        text = r.recognize_google(audio, language=lang)\n",
        "        print(\"You said:\", text)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio\")\n",
        "    except sr.RequestError as e:\n",
        "        print(\"API request error:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    listen_and_transcribe(\"en-US\")  # e.g., \"fa-IR\" for Persian\n",
        "```\n",
        "\n",
        "> Notes\n",
        "> • The free recognizer is rate-limited and not privacy-friendly for sensitive data. For production, use local Whisper, Vosk, or paid cloud APIs.\n",
        "> • Microphone level matters: avoid clipping; use `adjust_for_ambient_noise`.\n",
        "\n",
        "---\n",
        "\n",
        "# 3) Text-to-Speech (TTS) using Hugging Face **serverless** (Inference API)\n",
        "\n",
        "## What does “serverless” mean here?\n",
        "\n",
        "You **call a managed model endpoint** (Hugging Face Inference API). You don’t provision servers; HF handles scaling, GPU, and uptime. You pay per usage/plan. Pros: zero ops, quick start. Cons: network latency, request limits/cold starts, and you’re sending data off-box.\n",
        "\n",
        "### Install\n",
        "\n",
        "```bash\n",
        "pip install requests\n",
        "```\n",
        "\n",
        "### Code (send text → get a WAV file)\n",
        "\n",
        "Here we call a public TTS model via the Inference API. You need a **HF API token** (create one on your HF account, scope: “Read”).\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "HF_TOKEN = \"hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"  # <- put your token here\n",
        "MODEL_ID = \"suno/bark-small\"  # or \"facebook/mms-tts-eng\", etc.\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
        "    \"Accept\": \"audio/wav\"  # ask for raw audio back\n",
        "}\n",
        "\n",
        "payload = {\n",
        "    \"inputs\": \"Hello! This speech was generated with a serverless Hugging Face endpoint.\",\n",
        "    \"parameters\": {\n",
        "        # model-specific knobs; Bark can take e.g. speaker, tempo, etc.\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = requests.post(\n",
        "    f\"https://api-inference.huggingface.co/models/{MODEL_ID}\",\n",
        "    headers=headers,\n",
        "    json=payload,\n",
        "    timeout=120\n",
        ")\n",
        "resp.raise_for_status()\n",
        "\n",
        "with open(\"tts_output.wav\", \"wb\") as f:\n",
        "    f.write(resp.content)\n",
        "\n",
        "print(\"Saved: tts_output.wav\")\n",
        "```\n",
        "\n",
        "> Swap `MODEL_ID` for languages/voices (e.g., `facebook/mms-tts-eng` for English, other MMS TTS variants for different languages). Some models may warm up on the first call (slower the very first time).\n",
        "\n",
        "---\n",
        "\n",
        "# 4) “Tokenizer” vs “Processor” in audio\n",
        "\n",
        "On Hugging Face:\n",
        "\n",
        "* **Tokenizer**: text ↔ tokens (NLP).\n",
        "* **Feature extractor**: audio ↔ numeric features (e.g., log-mels).\n",
        "* **Processor**: **a wrapper that bundles both** (and sometimes normalizers) so you can feed/parse **audio+text** correctly.\n",
        "  Examples: `WhisperProcessor`, `Wav2Vec2Processor`, `SpeechT5Processor`.\n",
        "\n",
        "> So your intuition is right: for speech models we typically use a **Processor** instead of just a Tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "# 5) Embeddings, sample rate, and key audio concepts\n",
        "\n",
        "* **Sample rate**: Hz samples per second. Many ASR models expect **16 kHz** mono. Resample mismatches hurt accuracy.\n",
        "* **Features**:\n",
        "\n",
        "  * **Mel-spectrograms** or **MFCCs** summarize frequency content over time.\n",
        "  * **Speaker embeddings** (a.k.a. d-vectors/x-vectors) represent **voice identity**—used for verification and **voice cloning**.\n",
        "* **Voice activity detection (VAD)**: detects speech vs. silence; used for streaming and diarization.\n",
        "* **Latency tips**: stream in short chunks (e.g., 0.5–1.0 s), do on-the-fly VAD, and prefer models with streaming decoders.\n",
        "\n",
        "---\n",
        "\n",
        "# 6) Speech-to-Text, Text-to-Speech, Voice Cloning (deepfake)\n",
        "\n",
        "* **STT**: audio → text (e.g., Whisper, Wav2Vec2, cloud APIs).\n",
        "* **TTS**: text → audio (e.g., Bark, SpeechT5, VITS, FastPitch, FastSpeech2).\n",
        "* **Voice cloning**: generate TTS that **sounds like a specific speaker** using a short voice sample (few seconds to a minute).\n",
        "  ⚠️ **Ethics & consent**: only clone voices with explicit permission; label synthetic audio; be mindful of laws and platform policies.\n",
        "\n",
        "---\n",
        "\n",
        "# 7) Real-time voice cloning: encoder → synthesizer → vocoder\n",
        "\n",
        "Classic pipeline:\n",
        "\n",
        "1. **Speaker encoder**: turns a reference voice clip into a **speaker embedding**.\n",
        "2. **Text/Acoustic synthesizer**: predicts a **mel-spectrogram** from text + speaker embedding.\n",
        "3. **Vocoder**: converts the spectrogram into a **waveform** (Griffin-Lim, WaveGlow, HiFi-GAN, etc.).\n",
        "\n",
        "Modern “all-in-one” models hide these steps but the logic is the same.\n",
        "\n",
        "### Practical zero-shot cloning example (local) with Coqui-TTS\n",
        "\n",
        "This is a simple, production-friendly way to demo cloning on your own machine.\n",
        "\n",
        "#### Install\n",
        "\n",
        "```bash\n",
        "pip install TTS==0.22.0 torch soundfile\n",
        "# pick the right torch build for your GPU/OS if needed\n",
        "```\n",
        "\n",
        "#### Code (clone from a reference WAV, synthesize to file)\n",
        "\n",
        "```python\n",
        "from TTS.api import TTS\n",
        "\n",
        "# A popular zero-shot cloning model (multilingual):\n",
        "MODEL_NAME = \"tts_models/multilingual/multi-dataset/your_tts\"\n",
        "\n",
        "# Path to a short reference clip (5–15 seconds) of the target speaker, mono 16k or 22k works well\n",
        "SPEAKER_WAV = \"reference_speaker.wav\"\n",
        "\n",
        "tts = TTS(MODEL_NAME)\n",
        "\n",
        "text = \"This is a quick demo of zero-shot voice cloning.\"\n",
        "tts.tts_to_file(\n",
        "    text=text,\n",
        "    speaker_wav=SPEAKER_WAV,   # zero-shot cloning\n",
        "    file_path=\"cloned_voice.wav\"\n",
        ")\n",
        "\n",
        "print(\"Saved: cloned_voice.wav\")\n",
        "```\n",
        "\n",
        "> Tips\n",
        "> • Clean reference audio: no music, minimal noise, consistent mic distance.\n",
        "> • Try multiple 5–15s samples for better similarity.\n",
        "> • For **real-time**, you need **low-latency** models, fast vocoders (e.g., HiFi-GAN), and streaming chunks. True “live” cloning is possible but hardware-dependent.\n",
        "\n",
        "---\n",
        "\n",
        "# 8) Bonus: local, offline TTS with Transformers (no serverless)\n",
        "\n",
        "If you want everything on your machine (privacy, no network) you can do:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets torchaudio accelerate soundfile\n",
        "```\n",
        "\n",
        "```python\n",
        "import torch, soundfile as sf\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, AutoTokenizer\n",
        "\n",
        "text = \"Offline speech synthesis with SpeechT5.\"\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "inputs = processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "# Use a default speaker embedding (or learn/provide one for a target voice)\n",
        "# Here we just use a random embedding for demo:\n",
        "speaker_embeddings = torch.randn(1, 512)\n",
        "\n",
        "with torch.no_grad():\n",
        "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
        "\n",
        "sf.write(\"offline_tts.wav\", speech.numpy(), 16000)\n",
        "print(\"Saved: offline_tts.wav\")\n",
        "```\n",
        "\n",
        "> For cloning with SpeechT5 you’d feed a **real speaker embedding** (extracted with a compatible speaker-ID model) rather than random vectors.\n",
        "\n",
        "---\n",
        "\n",
        "# 9) Putting it together (mini recipes)\n",
        "\n",
        "**A) Live captioning**\n",
        "\n",
        "* Mic → VAD → stream 16 kHz chunks → streaming ASR (e.g., Whisper small w/ VAD) → display partial transcripts.\n",
        "\n",
        "**B) Talking assistant**\n",
        "\n",
        "* User speaks → STT → LLM → TTS (serverless for simplicity) → play audio. Cache TTS responses to reduce costs.\n",
        "\n",
        "**C) Real-time cloning**\n",
        "\n",
        "* Precompute speaker embedding (from 10–30 s sample).\n",
        "* For each user prompt: text → spectrogram (conditioned on embedding) → fast vocoder → stream audio frames.\n",
        "\n",
        "---\n",
        "\n",
        "# 10) Checklist & pitfalls\n",
        "\n",
        "* Match **sample rate** (usually 16 kHz for ASR).\n",
        "* Normalize loudness; avoid clipping.\n",
        "* Trim silences for better transcription and cloning.\n",
        "* Mind **latency** (smaller chunks, GPU if possible, lightweight models).\n",
        "* Legal/ethical: consent, disclosure, watermarking if applicable.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, say the word and I’ll package this as a printable PDF or a polished Markdown booklet with diagrams.\n"
      ],
      "metadata": {
        "id": "bByP11J22KtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiEvdBpU2KKW"
      },
      "outputs": [],
      "source": []
    }
  ]
}